import os
os.getcwd()
os.chdir('C:\\Users\\anuoo\\Desktop')
import pandas as pd 
pd.read_csv("Churn_Modelling.csv")
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	1	15634602	Hargrave	619	France	Female	42	2	0.00	1	1	1	101348.88	1
1	2	15647311	Hill	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
2	3	15619304	Onio	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
3	4	15701354	Boni	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	5	15737888	Mitchell	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
9995	9996	15606229	Obijiaku	771	France	Male	39	5	0.00	2	1	0	96270.64	0
9996	9997	15569892	Johnstone	516	France	Male	35	10	57369.61	1	1	1	101699.77	0
9997	9998	15584532	Liu	709	France	Female	36	7	0.00	1	0	1	42085.58	1
9998	9999	15682355	Sabbatini	772	Germany	Male	42	3	75075.31	2	1	0	92888.52	1
9999	10000	15628319	Walker	792	France	Female	28	4	130142.79	1	1	0	38190.78	0
10000 rows Ã— 14 columns

#df.tail()
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import warnings
warnings.filterwarnings("ignore")
sns.set(style="darkgrid",font_scale=1.5)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score ,f1_score
from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score
df = pd.read_csv("Churn_Modelling.csv")
df.head()
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
0	1	15634602	Hargrave	619	France	Female	42	2	0.00	1	1	1	101348.88	1
1	2	15647311	Hill	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
2	3	15619304	Onio	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
3	4	15701354	Boni	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	5	15737888	Mitchell	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0
df.columns
Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',
       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',
       'IsActiveMember', 'EstimatedSalary', 'Exited'],
      dtype='object')
print("Total number of records/rows present in the dataset is:",df.shape[0])
print("Total number of attributes/columns present in the dataset is:",df.shape[1])
Total number of records/rows present in the dataset is: 10000
Total number of attributes/columns present in the dataset is: 14
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 10000 entries, 0 to 9999
Data columns (total 14 columns):
 #   Column           Non-Null Count  Dtype  
---  ------           --------------  -----  
 0   RowNumber        10000 non-null  int64  
 1   CustomerId       10000 non-null  int64  
 2   Surname          10000 non-null  object 
 3   CreditScore      10000 non-null  int64  
 4   Geography        10000 non-null  object 
 5   Gender           10000 non-null  object 
 6   Age              10000 non-null  int64  
 7   Tenure           10000 non-null  int64  
 8   Balance          10000 non-null  float64
 9   NumOfProducts    10000 non-null  int64  
 10  HasCrCard        10000 non-null  int64  
 11  IsActiveMember   10000 non-null  int64  
 12  EstimatedSalary  10000 non-null  float64
 13  Exited           10000 non-null  int64  
dtypes: float64(2), int64(9), object(3)
memory usage: 1.1+ MB
df.isnull().sum().to_frame().rename(columns={0:"Total No. of Missing Values"})
Total No. of Missing Values
RowNumber	0
CustomerId	0
Surname	0
CreditScore	0
Geography	0
Gender	0
Age	0
Tenure	0
Balance	0
NumOfProducts	0
HasCrCard	0
IsActiveMember	0
EstimatedSalary	0
Exited	0
df[df.duplicated()]
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
df.describe(include="object").T
count	unique	top	freq
Surname	10000	2921	Lo	33
Geography	10000	3	France	5014
Gender	10000	2	Male	5457
df.sample(5)
RowNumber	CustomerId	Surname	CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Exited
4713	4714	15764448	Mackenzie	837	Germany	Male	35	0	144037.60	1	1	0	145325.32	0
7119	7120	15636478	Williams	621	France	Male	31	7	136658.61	1	1	1	148689.13	0
2496	2497	15625623	Stevenson	567	France	Female	45	4	0.00	2	0	1	121053.19	0
6107	6108	15737910	Houghton	703	Germany	Male	35	5	140691.08	2	1	0	167810.26	0
770	771	15584091	Pitts	742	Germany	Female	36	2	129748.54	2	0	0	47271.61	1
df.drop(columns=["RowNumber","CustomerId","Surname"],inplace=True)
df.rename(columns={"Exited":"Churned"},inplace=True)
df.head()
CreditScore	Geography	Gender	Age	Tenure	Balance	NumOfProducts	HasCrCard	IsActiveMember	EstimatedSalary	Churned
0	619	France	Female	42	2	0.00	1	1	1	101348.88	1
1	608	Spain	Female	41	1	83807.86	1	0	1	112542.58	0
2	502	France	Female	42	8	159660.80	3	1	0	113931.57	1
3	699	France	Female	39	1	0.00	2	0	0	93826.63	0
4	850	Spain	Female	43	2	125510.82	1	1	1	79084.10	0
#Explorator Data Analysis

count = df["Churned"].value_counts()

plt.figure(figsize=(14,6))
plt.subplot(1,2,1)
ax=sns.countplot(df["Churned"],palette="Set2")
ax.bar_label(ax.containers[0],fontweight="black",size=15)
plt.title("Customer Churned Disribution",fontweight="black",size=20,pad=20)

plt.subplot(1,2,2)
plt.pie(count.values, labels=count.index, autopct="%1.1f%%",colors=sns.set_palette("Set2"),
        textprops={"fontweight":"black"},explode=[0,0.1])
plt.title("Customer Churned Disribution",fontweight="black",size=20,pad=20)
plt.show()

def countplot(column):
    plt.figure(figsize=(15,5))
    ax = sns.countplot(x=column, data=df, hue="Churned",palette="Set2")
    for value in ax.patches:
        percentage = "{:.1f}%".format(100*value.get_height()/len(df[column]))
        x = value.get_x() + value.get_width() / 2 - 0.05
        y = value.get_y() + value.get_height()
        ax.annotate(percentage, (x,y), fontweight="black",size=15)
        
    plt.title(f"Customer Churned by {column}",fontweight="black",size=20,pad=20)
    plt.show()
countplot("Gender")

countplot("Geography")

countplot("HasCrCard")

countplot("NumOfProducts")

countplot("IsActiveMember")

plt.figure(figsize=(15,5))
ax = sns.countplot(x="Tenure", data=df, hue="Churned",palette="Set2")
for value in ax.patches:
    percentage = "{:.1f}%".format(100*value.get_height()/len(df["Tenure"]))
    x = value.get_x() + value.get_width() / 2 - 0.05
    y = value.get_y() + value.get_height()
    ax.annotate(percentage, (x,y), fontweight="black",size=12, ha="center")

plt.title("Customer Churned by Tenure",fontweight="black",size=20,pad=20)
plt.show()

def continous_plot(column):
    plt.figure(figsize=(13,6))
    plt.subplot(1,2,1)
    sns.histplot(x=column,hue="Churned",data=df,kde=True,palette="Set2")
    plt.title(f"Distribution of {column} by Churn Status",fontweight="black",pad=20,size=15)

    plt.subplot(1,2,2)
    sns.boxplot(df["Churned"],df[column],palette="Set2")
    plt.title(f"Distribution of {column} by Churn Status",fontweight="black",pad=20,size=15)
    plt.tight_layout()
    plt.show()
continous_plot("CreditScore")

continous_plot("Age")

continous_plot("Balance")

continous_plot("EstimatedSalary")

#Feature Engineering is the process of selecting, manipulating and transforming raw data into features that can be used in supervised learning


conditions = [(df["NumOfProducts"]==1), (df["NumOfProducts"]==2), (df["NumOfProducts"]>2)]
values =     ["One product","Two Products","More Than 2 Products"]
df["Total_Products"] = np.select(conditions,values)
df.drop(columns="NumOfProducts", inplace=True)
countplot("Total_Products")

conditions = [(df["Balance"]==0), (df["Balance"]>0)]
values = ["Zero Balance","More Than zero Balance"]
df["Account_Balance"] = np.select(conditions, values)
df.drop(columns="Balance",inplace=True)
countplot("Account_Balance")

#Data Preprocessing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset

cat_cols = ["Geography","Gender","Total_Products","Account_Balance"]

for column in cat_cols:
    print(f"Unique Values in {column} column is:",df[column].unique())
    print("-"*100,"\n")
Unique Values in Geography column is: ['France' 'Spain' 'Germany']
---------------------------------------------------------------------------------------------------- 

Unique Values in Gender column is: ['Female' 'Male']
---------------------------------------------------------------------------------------------------- 

Unique Values in Total_Products column is: ['One product' 'More Than 2 Products' 'Two Products']
---------------------------------------------------------------------------------------------------- 

Unique Values in Account_Balance column is: ['Zero Balance' 'More Than zero Balance']
---------------------------------------------------------------------------------------------------- 

df = pd.get_dummies(columns=cat_cols, data=df)
df["Churned"].replace({"No":0,"Yes":1},inplace=True)
df.head()
CreditScore	Age	Tenure	HasCrCard	IsActiveMember	EstimatedSalary	Churned	Geography_France	Geography_Germany	Geography_Spain	Gender_Female	Gender_Male	Total_Products_More Than 2 Products	Total_Products_One product	Total_Products_Two Products	Account_Balance_More Than zero Balance	Account_Balance_Zero Balance
0	619	42	2	1	1	101348.88	1	1	0	0	1	0	0	1	0	0	1
1	608	41	1	0	1	112542.58	0	0	0	1	1	0	0	1	0	1	0
2	502	42	8	1	0	113931.57	1	1	0	0	1	0	1	0	0	1	0
3	699	39	1	0	0	93826.63	0	1	0	0	1	0	0	0	1	0	1
4	850	43	2	1	1	79084.10	0	0	0	1	1	0	0	1	0	1	0
cols = ["CreditScore","Age","EstimatedSalary"]
df[cols].skew().to_frame().rename(columns={0:"Feature Skewness"})
Feature Skewness
CreditScore	-0.071607
Age	1.011320
EstimatedSalary	0.002085
old_age = df["Age"]     ##Storing the previous Age values to compare these values with the transformed values.
df["Age"] = np.log(df["Age"])
plt.figure(figsize=(13,6))
plt.subplot(1,2,1)
sns.histplot(old_age, color="purple", kde=True)
plt.title("Age Distribution Before Transformation",fontweight="black",size=18,pad=20)

plt.subplot(1,2,2)
sns.histplot(df["Age"], color="purple", kde=True)
plt.title("Age Distribution After Transformation",fontweight="black",size=18,pad=20)
plt.tight_layout()
plt.show()

! pip install --upgrade imbalanced-learn scikit-learn
Requirement already satisfied: imbalanced-learn in c:\users\anuoo\.ipython\extensions\lib\site-packages (0.12.2)
Requirement already satisfied: scikit-learn in c:\users\anuoo\.ipython\extensions\lib\site-packages (1.4.2)
Requirement already satisfied: numpy>=1.17.3 in c:\users\anuoo\.ipython\extensions\lib\site-packages (from imbalanced-learn) (1.24.3)
Requirement already satisfied: scipy>=1.5.0 in c:\users\anuoo\.ipython\extensions\lib\site-packages (from imbalanced-learn) (1.11.1)
Requirement already satisfied: joblib>=1.1.1 in c:\users\anuoo\.ipython\extensions\lib\site-packages (from imbalanced-learn) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in c:\users\anuoo\.ipython\extensions\lib\site-packages (from imbalanced-learn) (2.2.0)
from imblearn.over_sampling import SMOTE
X = df.drop(columns=["Churned"])
y = df["Churned"]
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)
print("Shape of x_train is:",x_train.shape)
print("Shape of x_test is: ",x_test.shape)
print("Shape of y_train is:",y_train.shape)
print("Shape of y_test is: ",y_test.shape)
Shape of x_train is: (8000, 16)
Shape of x_test is:  (2000, 16)
Shape of y_train is: (8000,)
Shape of y_test is:  (2000,)
smt = SMOTE(random_state=42)
x_train_resampled,y_train_resampled = smt.fit_resample(x_train,y_train)
print(x_train_resampled.shape ,y_train_resampled.shape)
(12736, 16) (12736,)
y_train_resampled.value_counts().to_frame()
Churned
0	6368
1	6368
dtree = DecisionTreeClassifier()
param_grid = {"max_depth":[3,4,5,6,7,8,9,10],
              "min_samples_split":[2,3,4,5,6,7,8],
              "min_samples_leaf":[1,2,3,4,5,6,7,8],
              "criterion":["gini","entropy"],
              "splitter":["best","random"],
              "max_features":["auto",None],
              "random_state":[0,42]}
grid_search = GridSearchCV(dtree, param_grid, cv=5, n_jobs=-1)

grid_search.fit(x_train_resampled,y_train_resampled)
  GridSearchCV?i
estimator: DecisionTreeClassifier

 DecisionTreeClassifier?
best_parameters = grid_search.best_params_

print("Best Parameters for DecisionTree Model is:\n\n")
best_parameters
Best Parameters for DecisionTree Model is:


{'criterion': 'gini',
 'max_depth': 8,
 'max_features': None,
 'min_samples_leaf': 1,
 'min_samples_split': 7,
 'random_state': 42,
 'splitter': 'random'}
dtree = DecisionTreeClassifier(**best_parameters)

dtree.fit(x_train_resampled,y_train_resampled)

  DecisionTreeClassifier?i
DecisionTreeClassifier(max_depth=8, min_samples_split=7, random_state=42,
                       splitter='random')
y_train_pred = dtree.predict(x_train_resampled)
y_test_pred = dtree.predict(x_test)

print("Accuracy Score of Model on Training Data is =>",round(accuracy_score(y_train_resampled,y_train_pred)*100,2),"%")
print("Accuracy Score of Model on Testing Data  is =>",round(accuracy_score(y_test,y_test_pred)*100,2),"%")
Accuracy Score of Model on Training Data is => 89.51 %
Accuracy Score of Model on Testing Data  is => 84.3 %
print("F1 Score of the Model is =>",f1_score(y_test,y_test_pred,average="micro"))
print("Recall Score of the Model is =>",recall_score(y_test,y_test_pred,average="micro"))
print("Precision Score of the Model is =>",precision_score(y_test,y_test_pred,average="micro"))
F1 Score of the Model is => 0.843
Recall Score of the Model is => 0.843
Precision Score of the Model is => 0.843
imp_df = pd.DataFrame({"Feature Name":x_train.columns,
                       "Importance":dtree.feature_importances_})
features = imp_df.sort_values(by="Importance",ascending=False)

plt.figure(figsize=(12,7))
sns.barplot(x="Importance", y="Feature Name", data=features, palette="plasma")
plt.title("Feature Importance in the Model Prediction", fontweight="black", size=20, pad=20)
plt.yticks(size=12)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load data
data = pd.read_csv('Churn_Modelling.csv')  # replace with your file path

# Feature selection and preprocessing
X = data[['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance',
          'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']]
y = data['Exited']

# Convert categorical variables to dummy variables
X = pd.get_dummies(X, drop_first=True)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_log_reg))

# Decision Tree
dec_tree = DecisionTreeClassifier(random_state=42)
dec_tree.fit(X_train, y_train)
y_pred_dec_tree = dec_tree.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dec_tree))
print("Decision Tree Classification Report:\n", classification_report(y_test, y_pred_dec_tree))

# Random Forest
rand_forest = RandomForestClassifier(random_state=42)
rand_forest.fit(X_train, y_train)
y_pred_rand_forest = rand_forest.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rand_forest))
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rand_forest))

# Gradient Boosting
grad_boost = GradientBoostingClassifier(random_state=42)
grad_boost.fit(X_train, y_train)
y_pred_grad_boost = grad_boost.predict(X_test)
print("Gradient Boosting Accuracy:", accuracy_score(y_test, y_pred_grad_boost))
print("Gradient Boosting Classification Report:\n", classification_report(y_test, y_pred_grad_boost))
Logistic Regression Accuracy: 0.811
Logistic Regression Classification Report:
               precision    recall  f1-score   support

           0       0.83      0.96      0.89      1607
           1       0.55      0.20      0.29       393

    accuracy                           0.81      2000
   macro avg       0.69      0.58      0.59      2000
weighted avg       0.78      0.81      0.77      2000

Decision Tree Accuracy: 0.781
Decision Tree Classification Report:
               precision    recall  f1-score   support

           0       0.87      0.85      0.86      1607
           1       0.45      0.50      0.47       393

    accuracy                           0.78      2000
   macro avg       0.66      0.68      0.67      2000
weighted avg       0.79      0.78      0.79      2000

Random Forest Accuracy: 0.8665
Random Forest Classification Report:
               precision    recall  f1-score   support

           0       0.88      0.96      0.92      1607
           1       0.76      0.47      0.58       393

    accuracy                           0.87      2000
   macro avg       0.82      0.72      0.75      2000
weighted avg       0.86      0.87      0.85      2000

Gradient Boosting Accuracy: 0.8675
Gradient Boosting Classification Report:
               precision    recall  f1-score   support

           0       0.88      0.96      0.92      1607
           1       0.75      0.49      0.59       393

    accuracy                           0.87      2000
   macro avg       0.82      0.72      0.76      2000
weighted avg       0.86      0.87      0.86      2000

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load data
data = pd.read_csv('Churn_Modelling.csv')  # replace with your file path

# Feature selection and preprocessing
X = data[['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance',
          'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']]
y = data['Exited']

# Convert categorical variables to dummy variables
X = pd.get_dummies(X, drop_first=True)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression hyperparameter tuning
log_reg_params = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}
log_reg = LogisticRegression(random_state=42)
log_reg_cv = GridSearchCV(log_reg, log_reg_params, cv=5)
log_reg_cv.fit(X_train, y_train)
print("Best Logistic Regression Parameters:", log_reg_cv.best_params_)
y_pred_log_reg = log_reg_cv.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log_reg))
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_log_reg))

# Decision Tree hyperparameter tuning
dec_tree_params = {
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}
dec_tree = DecisionTreeClassifier(random_state=42)
dec_tree_cv = GridSearchCV(dec_tree, dec_tree_params, cv=5)
dec_tree_cv.fit(X_train, y_train)
print("Best Decision Tree Parameters:", dec_tree_cv.best_params_)
y_pred_dec_tree = dec_tree_cv.predict(X_test)
print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dec_tree))
print("Decision Tree Classification Report:\n", classification_report(y_test, y_pred_dec_tree))

# Random Forest hyperparameter tuning
rand_forest_params = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}
rand_forest = RandomForestClassifier(random_state=42)
rand_forest_cv = GridSearchCV(rand_forest, rand_forest_params, cv=5)
rand_forest_cv.fit(X_train, y_train)
print("Best Random Forest Parameters:", rand_forest_cv.best_params_)
y_pred_rand_forest = rand_forest_cv.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rand_forest))
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rand_forest))

# Gradient Boosting hyperparameter tuning
grad_boost_params = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5],
    'subsample': [0.8, 0.9, 1.0],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grad_boost = GradientBoostingClassifier(random_state=42)
grad_boost_cv = GridSearchCV(grad_boost, grad_boost_params, cv=5)
grad_boost_cv.fit(X_train, y_train)
print("Best Gradient Boosting Parameters:", grad_boost_cv.best_params_)
y_pred_grad_boost = grad_boost_cv.predict(X_test)
print("Gradient Boosting Accuracy:", accuracy_score(y_test, y_pred_grad_boost))
print("Gradient Boosting Classification Report:\n", classification_report(y_test, y_pred_grad_boost))
